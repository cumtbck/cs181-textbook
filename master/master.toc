\contentsline {chapter}{\numberline {1}Introduction to Machine Learning}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}What is Machine Learning?}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}What Will This Book Teach Me?}{1}{section.1.2}%
\contentsline {section}{\numberline {1.3}Our Machine Learning Framework}{2}{section.1.3}%
\contentsline {section}{\numberline {1.4}This Book's Notation}{3}{section.1.4}%
\contentsline {subsubsection}{Mathematical and Statistical Notation}{3}{section*.2}%
\contentsline {subsubsection}{Textbook Specific Notation}{3}{section*.3}%
\contentsline {chapter}{\numberline {2}Regression}{4}{chapter.2}%
\contentsline {section}{\numberline {2.1}Defining the Problem}{4}{section.2.1}%
\contentsline {section}{\numberline {2.2}Solution Options}{4}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}K-Nearest-Neighbors}{5}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Neural Networks}{5}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Random Forests}{5}{subsection.2.2.3}%
\contentsline {subsection}{\numberline {2.2.4}Gradient Boosted Trees}{5}{subsection.2.2.4}%
\contentsline {subsection}{\numberline {2.2.5}Turning to Linear Regression}{5}{subsection.2.2.5}%
\contentsline {section}{\numberline {2.3}Introduction to Linear Regression}{6}{section.2.3}%
\contentsline {section}{\numberline {2.4}Basic Setup}{6}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Merging of Bias}{7}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Visualization of Linear Regression}{7}{subsection.2.4.2}%
\contentsline {section}{\numberline {2.5}Finding the Best Fitting Line}{8}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}Objective Functions and Loss}{8}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}Least Squares Loss}{9}{subsection.2.5.2}%
\contentsline {section}{\numberline {2.6}Linear Regression Algorithms}{10}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}Optimal Weights via Matrix Differentiation}{10}{subsection.2.6.1}%
\contentsline {subsection}{\numberline {2.6.2}Bayesian Solution: Maximum Likelihood Estimation}{11}{subsection.2.6.2}%
\contentsline {subsection}{\numberline {2.6.3}Alternate Interpretation: Linear Regression as Projection}{13}{subsection.2.6.3}%
\contentsline {section}{\numberline {2.7}Model Flexibility}{13}{section.2.7}%
\contentsline {subsection}{\numberline {2.7.1}Basis Functions}{13}{subsection.2.7.1}%
\contentsline {subsection}{\numberline {2.7.2}Regularization}{16}{subsection.2.7.2}%
\contentsline {subsection}{\numberline {2.7.3}Generalizing Regularization}{19}{subsection.2.7.3}%
\contentsline {subsection}{\numberline {2.7.4}Bayesian Regularization}{20}{subsection.2.7.4}%
\contentsline {section}{\numberline {2.8}Choosing Between Models}{22}{section.2.8}%
\contentsline {subsection}{\numberline {2.8.1}Bias-Variance Tradeoff and Decomposition}{22}{subsection.2.8.1}%
\contentsline {subsection}{\numberline {2.8.2}Cross-Validation}{26}{subsection.2.8.2}%
\contentsline {subsection}{\numberline {2.8.3}Making a Model Choice}{26}{subsection.2.8.3}%
\contentsline {subsection}{\numberline {2.8.4}Bayesian Model Averaging}{27}{subsection.2.8.4}%
\contentsline {section}{\numberline {2.9}Linear Regression Extras}{27}{section.2.9}%
\contentsline {subsection}{\numberline {2.9.1}Predictive Distribution}{27}{subsection.2.9.1}%
\contentsline {section}{\numberline {2.10}Conclusion}{28}{section.2.10}%
\contentsline {chapter}{\numberline {3}Classification}{29}{chapter.3}%
\contentsline {section}{\numberline {3.1}Defining the Problem}{29}{section.3.1}%
\contentsline {section}{\numberline {3.2}Solution Options}{29}{section.3.2}%
\contentsline {section}{\numberline {3.3}Discriminant Functions}{30}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Basic Setup: Binary Linear Classification}{30}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Multiple Classes}{31}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Basis Changes in Classification}{33}{subsection.3.3.3}%
\contentsline {section}{\numberline {3.4}Numerical Parameter Optimization and Gradient Descent}{33}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Gradient Descent}{35}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Batch Gradient Descent versus Stochastic Gradient Descent}{35}{subsection.3.4.2}%
\contentsline {section}{\numberline {3.5}Loss Functions for Decision Boundaries}{36}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1}0/1 Loss}{36}{subsection.3.5.1}%
\contentsline {subsection}{\numberline {3.5.2}Least Squares Loss}{37}{subsection.3.5.2}%
\contentsline {subsection}{\numberline {3.5.3}Hinge Loss}{37}{subsection.3.5.3}%
\contentsline {section}{\numberline {3.6}Probabilistic Methods}{39}{section.3.6}%
\contentsline {subsection}{\numberline {3.6.1}Probabilistic Discriminative Models}{39}{subsection.3.6.1}%
\contentsline {subsubsection}{Logistic Regression}{39}{section*.4}%
\contentsline {subsubsection}{Multi-Class Logistic Regression and Softmax}{42}{section*.5}%
\contentsline {subsection}{\numberline {3.6.2}Probabilistic Generative Models}{43}{subsection.3.6.2}%
\contentsline {subsubsection}{Classification in the Generative Setting}{43}{section*.6}%
\contentsline {subsubsection}{MLE Solution}{45}{section*.7}%
\contentsline {subsubsection}{Naive Bayes}{46}{section*.8}%
\contentsline {section}{\numberline {3.7}Conclusion}{48}{section.3.7}%
\contentsline {chapter}{\numberline {4}Neural Networks}{49}{chapter.4}%
\contentsline {section}{\numberline {4.1}Motivation}{49}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Comparison to Other Methods}{50}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Universal Function Approximation}{50}{subsection.4.1.2}%
\contentsline {section}{\numberline {4.2}Feed-Forward Networks}{51}{section.4.2}%
\contentsline {section}{\numberline {4.3}Neural Network Basics and Terminology}{51}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Adaptive Basis Functions}{52}{subsection.4.3.1}%
\contentsline {section}{\numberline {4.4}Network Training}{55}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Objective Function}{55}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Optimizing Parameters}{55}{subsection.4.4.2}%
\contentsline {subsection}{\numberline {4.4.3}Backpropagation}{56}{subsection.4.4.3}%
\contentsline {subsection}{\numberline {4.4.4}Computing Derivatives Using Backpropagation}{56}{subsection.4.4.4}%
\contentsline {section}{\numberline {4.5}Choosing a Network Structure}{60}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}Cross Validation for Neural Networks}{60}{subsection.4.5.1}%
\contentsline {subsection}{\numberline {4.5.2}Preventing Overfitting}{61}{subsection.4.5.2}%
\contentsline {subsubsection}{Regularization}{61}{section*.9}%
\contentsline {subsubsection}{Data Augmentation}{61}{section*.10}%
\contentsline {section}{\numberline {4.6}Specialized Forms of Neural Networks}{62}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}Convolutional Neural Networks (CNNs)}{62}{subsection.4.6.1}%
\contentsline {subsection}{\numberline {4.6.2}Recurrent Neural Networks (RNNs)}{62}{subsection.4.6.2}%
\contentsline {subsection}{\numberline {4.6.3}Bayesian Neural Networks (BNNs)}{63}{subsection.4.6.3}%
\contentsline {chapter}{\numberline {5}Support Vector Machines}{64}{chapter.5}%
\contentsline {section}{\numberline {5.1}Motivation}{64}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Max Margin Methods}{64}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Applications}{65}{subsection.5.1.2}%
\contentsline {section}{\numberline {5.2}Hard Margin Classifier for Linearly Separable Data}{66}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Why the Hard Margin}{66}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}Deriving our Optimization Problem}{66}{subsection.5.2.2}%
\contentsline {subsection}{\numberline {5.2.3}What is a Support Vector}{68}{subsection.5.2.3}%
\contentsline {section}{\numberline {5.3}Soft Margin Classifier}{69}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Why the Soft Margin?}{69}{subsection.5.3.1}%
\contentsline {subsection}{\numberline {5.3.2}Updated Optimization Problem for Soft Margins}{70}{subsection.5.3.2}%
\contentsline {subsection}{\numberline {5.3.3}Soft Margin Support Vectors}{71}{subsection.5.3.3}%
\contentsline {section}{\numberline {5.4}Conversion to Dual Form}{71}{section.5.4}%
\contentsline {subsection}{\numberline {5.4.1}Lagrange Multipliers}{72}{subsection.5.4.1}%
\contentsline {subsection}{\numberline {5.4.2}Deriving the Dual Formulation}{73}{subsection.5.4.2}%
\contentsline {subsection}{\numberline {5.4.3}Making Predictions}{74}{subsection.5.4.3}%
\contentsline {subsection}{\numberline {5.4.4}Why is the Dual Formulation Helpful?}{75}{subsection.5.4.4}%
\contentsline {subsection}{\numberline {5.4.5}Kernel Composition}{76}{subsection.5.4.5}%
\contentsline {chapter}{\numberline {6}Clustering}{77}{chapter.6}%
\contentsline {section}{\numberline {6.1}Motivation}{77}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}Applications}{78}{subsection.6.1.1}%
\contentsline {section}{\numberline {6.2}K-Means Clustering}{78}{section.6.2}%
\contentsline {subsection}{\numberline {6.2.1}Lloyd's Algorithm}{78}{subsection.6.2.1}%
\contentsline {subsection}{\numberline {6.2.2}Example of Lloyd's}{80}{subsection.6.2.2}%
\contentsline {subsection}{\numberline {6.2.3}Number of Clusters}{83}{subsection.6.2.3}%
\contentsline {subsection}{\numberline {6.2.4}Initialization and K-Means++}{83}{subsection.6.2.4}%
\contentsline {subsection}{\numberline {6.2.5}K-Medoids Alternative}{85}{subsection.6.2.5}%
\contentsline {section}{\numberline {6.3}Hierarchical Agglomerative Clustering}{85}{section.6.3}%
\contentsline {subsection}{\numberline {6.3.1}HAC Algorithm}{86}{subsection.6.3.1}%
\contentsline {subsection}{\numberline {6.3.2}Linkage Criterion}{88}{subsection.6.3.2}%
\contentsline {subsubsection}{Min-Linkage Criteria}{88}{section*.12}%
\contentsline {subsubsection}{Max-Linkage Criterion}{88}{section*.13}%
\contentsline {subsubsection}{Average-Linkage Criterion}{89}{section*.14}%
\contentsline {subsubsection}{Centroid-Linkage Criterion}{89}{section*.15}%
\contentsline {subsubsection}{Different Linkage Criteria Produce Different Clusterings}{89}{section*.16}%
\contentsline {subsection}{\numberline {6.3.3}How HAC Differs from K-Means}{90}{subsection.6.3.3}%
\contentsline {chapter}{\numberline {7}Dimensionality Reduction}{91}{chapter.7}%
\contentsline {section}{\numberline {7.1}Motivation}{91}{section.7.1}%
\contentsline {section}{\numberline {7.2}Applications}{92}{section.7.2}%
\contentsline {section}{\numberline {7.3}Principal Component Analysis}{92}{section.7.3}%
\contentsline {subsection}{\numberline {7.3.1}Reconstruction Loss}{93}{subsection.7.3.1}%
\contentsline {subsection}{\numberline {7.3.2}Minimizing Reconstruction Loss}{95}{subsection.7.3.2}%
\contentsline {subsection}{\numberline {7.3.3}Multiple Principal Components}{96}{subsection.7.3.3}%
\contentsline {subsection}{\numberline {7.3.4}Identifying Directions of Maximal Variance in our Data}{96}{subsection.7.3.4}%
\contentsline {subsection}{\numberline {7.3.5}Choosing the Optimal Number of Principal Components}{97}{subsection.7.3.5}%
\contentsline {section}{\numberline {7.4}Conclusion}{100}{section.7.4}%
\contentsline {chapter}{\numberline {8}Graphical Models}{101}{chapter.8}%
\contentsline {section}{\numberline {8.1}Motivation}{101}{section.8.1}%
\contentsline {section}{\numberline {8.2}Directed Graphical Models (Bayesian Networks)}{101}{section.8.2}%
\contentsline {subsection}{\numberline {8.2.1}Joint Probability Distributions}{104}{subsection.8.2.1}%
\contentsline {subsection}{\numberline {8.2.2}Generative Models}{104}{subsection.8.2.2}%
\contentsline {subsection}{\numberline {8.2.3}Generative Modeling vs. Discriminative Modeling}{106}{subsection.8.2.3}%
\contentsline {subsection}{\numberline {8.2.4}Understanding Complexity}{107}{subsection.8.2.4}%
\contentsline {subsection}{\numberline {8.2.5}Independence and D-Separation}{108}{subsection.8.2.5}%
\contentsline {section}{\numberline {8.3}Example: Naive Bayes}{111}{section.8.3}%
\contentsline {section}{\numberline {8.4}Conclusion}{111}{section.8.4}%
\contentsline {chapter}{\numberline {9}Mixture Models}{112}{chapter.9}%
\contentsline {section}{\numberline {9.1}Motivation}{112}{section.9.1}%
\contentsline {section}{\numberline {9.2}Applications}{114}{section.9.2}%
\contentsline {section}{\numberline {9.3}Fitting a Model}{114}{section.9.3}%
\contentsline {subsection}{\numberline {9.3.1}Maximum Likelihood for Mixture Models}{114}{subsection.9.3.1}%
\contentsline {subsection}{\numberline {9.3.2}Complete-Data Log Likelihood}{115}{subsection.9.3.2}%
\contentsline {section}{\numberline {9.4}Expectation-Maximization (EM)}{115}{section.9.4}%
\contentsline {subsection}{\numberline {9.4.1}Expectation Step}{116}{subsection.9.4.1}%
\contentsline {subsection}{\numberline {9.4.2}Maximization Step}{117}{subsection.9.4.2}%
\contentsline {subsection}{\numberline {9.4.3}Full EM Algorithm}{118}{subsection.9.4.3}%
\contentsline {subsection}{\numberline {9.4.4}The Math of EM}{118}{subsection.9.4.4}%
\contentsline {subsubsection}{The Evidence Lower Bound (ELBO)}{119}{section*.17}%
\contentsline {subsubsection}{Optimization}{120}{section*.18}%
\contentsline {subsubsection}{Correctness}{122}{section*.19}%
\contentsline {subsubsection}{Equivalence to Prior Formulation}{122}{section*.20}%
\contentsline {subsection}{\numberline {9.4.5}Connection to K-Means Clustering}{122}{subsection.9.4.5}%
\contentsline {subsection}{\numberline {9.4.6}Dice Example: Mixture of Multinomials}{123}{subsection.9.4.6}%
\contentsline {section}{\numberline {9.5}Gaussian Mixture Models (GMM)}{125}{section.9.5}%
\contentsline {section}{\numberline {9.6}Admixture Models: Latent Dirichlet Allocation (LDA)}{126}{section.9.6}%
\contentsline {subsection}{\numberline {9.6.1}LDA for Topic Modeling}{126}{subsection.9.6.1}%
\contentsline {subsection}{\numberline {9.6.2}Applying EM to LDA}{127}{subsection.9.6.2}%
\contentsline {section}{\numberline {9.7}Conclusion}{129}{section.9.7}%
\contentsline {chapter}{\numberline {10}Hidden Markov Models}{130}{chapter.10}%
\contentsline {section}{\numberline {10.1}Motivation}{130}{section.10.1}%
\contentsline {section}{\numberline {10.2}Applications}{131}{section.10.2}%
\contentsline {section}{\numberline {10.3}HMM Data, Model, and Parameterization}{132}{section.10.3}%
\contentsline {subsection}{\numberline {10.3.1}HMM Data}{132}{subsection.10.3.1}%
\contentsline {subsection}{\numberline {10.3.2}HMM Model Assumptions}{132}{subsection.10.3.2}%
\contentsline {subsection}{\numberline {10.3.3}HMM Parameterization}{133}{subsection.10.3.3}%
\contentsline {section}{\numberline {10.4}Inference in HMMs}{133}{section.10.4}%
\contentsline {subsection}{\numberline {10.4.1} The Forward-Backward Algorithm}{134}{subsection.10.4.1}%
\contentsline {subsection}{\numberline {10.4.2}Using $\alpha $'s and $\beta $'s for Training and Inference}{137}{subsection.10.4.2}%
\contentsline {subsubsection}{p(Seq)}{137}{section*.21}%
\contentsline {subsubsection}{Prediction}{137}{section*.22}%
\contentsline {subsubsection}{Smoothing}{137}{section*.23}%
\contentsline {subsubsection}{Transition}{138}{section*.24}%
\contentsline {subsubsection}{Filtering}{138}{section*.25}%
\contentsline {subsubsection}{Best path}{138}{section*.26}%
\contentsline {section}{\numberline {10.5}Using EM to Train a HMM}{139}{section.10.5}%
\contentsline {subsection}{\numberline {10.5.1}E-Step}{139}{subsection.10.5.1}%
\contentsline {subsection}{\numberline {10.5.2}M-Step}{140}{subsection.10.5.2}%
\contentsline {section}{\numberline {10.6}Conclusion}{140}{section.10.6}%
\contentsline {chapter}{\numberline {11}Markov Decision Processes}{141}{chapter.11}%
\contentsline {section}{\numberline {11.1}Formal Definition of an MDP}{142}{section.11.1}%
\contentsline {section}{\numberline {11.2}Finite Horizon Planning}{143}{section.11.2}%
\contentsline {section}{\numberline {11.3}Infinite Horizon Planning}{143}{section.11.3}%
\contentsline {subsection}{\numberline {11.3.1}Value iteration}{144}{subsection.11.3.1}%
\contentsline {subsubsection}{Bellman Consistency Equation and Bellman Optimality}{144}{section*.27}%
\contentsline {subsubsection}{Bellman Operator}{146}{section*.28}%
\contentsline {subsubsection}{Value Iteration Algorithm}{146}{section*.29}%
\contentsline {subsection}{\numberline {11.3.2}Policy Iteration}{147}{subsection.11.3.2}%
\contentsline {subsubsection}{Policy Evaluation}{147}{section*.30}%
\contentsline {chapter}{\numberline {12}Reinforcement Learning}{148}{chapter.12}%
\contentsline {section}{\numberline {12.1}Motivation}{148}{section.12.1}%
\contentsline {section}{\numberline {12.2}General Approaches to RL}{148}{section.12.2}%
\contentsline {section}{\numberline {12.3}Model-Free Learning}{149}{section.12.3}%
\contentsline {subsection}{\numberline {12.3.1}SARSA and Q-Learning}{150}{subsection.12.3.1}%
\contentsline {subsubsection}{Convergence Conditions}{151}{section*.31}%
\contentsline {subsection}{\numberline {12.3.2}Deep Q-Networks}{151}{subsection.12.3.2}%
\contentsline {subsection}{\numberline {12.3.3}Policy Learning}{152}{subsection.12.3.3}%
\contentsline {section}{\numberline {12.4}Model-Based Learning}{153}{section.12.4}%
\contentsline {section}{\numberline {12.5}Conclusion}{154}{section.12.5}%
